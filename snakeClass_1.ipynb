{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\achyu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\achyu\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WINDOWS\\system32\\Class\\Fall 2019\\Reinforcement Learning and Adaptive Control ECEN 5008\\Project\\snake-ga-master\\DQN_1.py:78: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=120)`\n",
      "  model.add(Dense(output_dim=120, activation='relu', input_dim=11))\n",
      "C:\\WINDOWS\\system32\\Class\\Fall 2019\\Reinforcement Learning and Adaptive Control ECEN 5008\\Project\\snake-ga-master\\DQN_1.py:80: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=120)`\n",
      "  model.add(Dense(output_dim=120, activation='relu'))\n",
      "C:\\WINDOWS\\system32\\Class\\Fall 2019\\Reinforcement Learning and Adaptive Control ECEN 5008\\Project\\snake-ga-master\\DQN_1.py:82: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=120)`\n",
      "  model.add(Dense(output_dim=120, activation='relu'))\n",
      "C:\\WINDOWS\\system32\\Class\\Fall 2019\\Reinforcement Learning and Adaptive Control ECEN 5008\\Project\\snake-ga-master\\DQN_1.py:84: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=3)`\n",
      "  model.add(Dense(output_dim=3, activation='softmax'))\n",
      "C:\\WINDOWS\\system32\\Class\\Fall 2019\\Reinforcement Learning and Adaptive Control ECEN 5008\\Project\\snake-ga-master\\DQN_1.py:78: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=120)`\n",
      "  model.add(Dense(output_dim=120, activation='relu', input_dim=11))\n",
      "C:\\WINDOWS\\system32\\Class\\Fall 2019\\Reinforcement Learning and Adaptive Control ECEN 5008\\Project\\snake-ga-master\\DQN_1.py:80: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=120)`\n",
      "  model.add(Dense(output_dim=120, activation='relu'))\n",
      "C:\\WINDOWS\\system32\\Class\\Fall 2019\\Reinforcement Learning and Adaptive Control ECEN 5008\\Project\\snake-ga-master\\DQN_1.py:82: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=120)`\n",
      "  model.add(Dense(output_dim=120, activation='relu'))\n",
      "C:\\WINDOWS\\system32\\Class\\Fall 2019\\Reinforcement Learning and Adaptive Control ECEN 5008\\Project\\snake-ga-master\\DQN_1.py:84: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=3)`\n",
      "  model.add(Dense(output_dim=3, activation='softmax'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "WARNING:tensorflow:From C:\\Users\\achyu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2fc8335b6011>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-2fc8335b6011>\u001b[0m in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m         \u001b[0mcounter_games\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;31m#print('Game', counter_games, '      Score:', game.score)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WINDOWS\\system32\\Class\\Fall 2019\\Reinforcement Learning and Adaptive Control ECEN 5008\\Project\\snake-ga-master\\DQN_1.py\u001b[0m in \u001b[0;36mreplay_new\u001b[1;34m(self, memory)\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0mtarget_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from random import randint\n",
    "from DQN_1 import DQNAgent\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from numba import vectorize\n",
    "\n",
    "\n",
    "# Set options to activate or deactivate the game view, and its speed\n",
    "display_option = True\n",
    "speed = 0\n",
    "pygame.font.init()\n",
    "\n",
    "\n",
    "class Game:\n",
    "\n",
    "    def __init__(self, game_width, game_height):\n",
    "        pygame.display.set_caption('SnakeGen')\n",
    "        self.game_width = game_width\n",
    "        self.game_height = game_height\n",
    "        self.gameDisplay = pygame.display.set_mode((game_width, game_height+60))\n",
    "        self.bg = pygame.image.load(\"img/background.png\")\n",
    "        self.bg_1 = pygame.image.load(\"img/wall_1.jpg\")\n",
    "        self.crash = False\n",
    "        self.player = Player(self)\n",
    "        self.food = Food()\n",
    "        self.score = 0\n",
    "\n",
    "\n",
    "class Player(object):\n",
    "\n",
    "    def __init__(self, game):\n",
    "        x = 0.45 * game.game_width\n",
    "        y = 0.5 * game.game_height\n",
    "        self.x = x - x % 20\n",
    "        self.y = y - y % 20\n",
    "        self.position = []\n",
    "        self.position.append([self.x, self.y])\n",
    "        self.food = 1\n",
    "        self.eaten = False\n",
    "        self.image = pygame.image.load('img/snakeBody_1.png')\n",
    "        self.x_change = 20\n",
    "        self.y_change = 0\n",
    "\n",
    "    def update_position(self, x, y):\n",
    "        if self.position[-1][0] != x or self.position[-1][1] != y:\n",
    "            if self.food > 1:\n",
    "                for i in range(0, self.food - 1):\n",
    "                    self.position[i][0], self.position[i][1] = self.position[i + 1]\n",
    "            self.position[-1][0] = x\n",
    "            self.position[-1][1] = y\n",
    "\n",
    "    def do_move(self, move, x, y, game, food,agent):\n",
    "        move_array = [self.x_change, self.y_change]\n",
    "\n",
    "        if self.eaten:\n",
    "\n",
    "            self.position.append([self.x, self.y])\n",
    "            self.eaten = False\n",
    "            self.food = self.food + 1\n",
    "        if np.array_equal(move ,[1, 0, 0]):\n",
    "            move_array = self.x_change, self.y_change\n",
    "        elif np.array_equal(move,[0, 1, 0]) and self.y_change == 0:  # right - going horizontal\n",
    "            move_array = [0, self.x_change]\n",
    "        elif np.array_equal(move,[0, 1, 0]) and self.x_change == 0:  # right - going vertical\n",
    "            move_array = [-self.y_change, 0]\n",
    "        elif np.array_equal(move, [0, 0, 1]) and self.y_change == 0:  # left - going horizontal\n",
    "            move_array = [0, -self.x_change]\n",
    "        elif np.array_equal(move,[0, 0, 1]) and self.x_change == 0:  # left - going vertical\n",
    "            move_array = [self.y_change, 0]\n",
    "        self.x_change, self.y_change = move_array\n",
    "        self.x = x + self.x_change\n",
    "        self.y = y + self.y_change\n",
    "\n",
    "        if self.x < 20 or self.x > game.game_width-40 or self.y < 20 or self.y > game.game_height-40 or [self.x, self.y] in self.position or (self.x<260 and self.y>60 and self.y<140):\n",
    "            game.crash = True\n",
    "        eat(self, food, game)\n",
    "\n",
    "        self.update_position(self.x, self.y)\n",
    "\n",
    "    def display_player(self, x, y, food, game):\n",
    "        self.position[-1][0] = x\n",
    "        self.position[-1][1] = y\n",
    "\n",
    "        if game.crash == False:\n",
    "            for i in range(food):\n",
    "                x_temp, y_temp = self.position[len(self.position) - 1 - i]\n",
    "                game.gameDisplay.blit(self.image, (x_temp, y_temp))\n",
    "\n",
    "            update_screen()\n",
    "        else:\n",
    "            pygame.time.wait(300)\n",
    "\n",
    "\n",
    "class Food(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x_food = 240\n",
    "        self.y_food = 200\n",
    "        self.image = pygame.image.load('img/food3.png')\n",
    "\n",
    "    def food_coord(self, game, player):\n",
    "        x_rand = randint(20, game.game_width - 40)\n",
    "        self.x_food = x_rand - x_rand % 20\n",
    "        while self.x_food in [0,20,40,60,80,100,120,140,160,180,200,220,240,260]:\n",
    "            x_rand = randint(20, game.game_width - 40)\n",
    "            self.x_food = x_rand - x_rand % 20\n",
    "        y_rand = randint(20, game.game_height - 40)\n",
    "        self.y_food = y_rand - y_rand % 20\n",
    "        while self.y_food in [60,80,100,120,140]:\n",
    "            y_rand = randint(20, game.game_width - 40)\n",
    "            self.y_food = y_rand - y_rand % 20\n",
    "        if [self.x_food, self.y_food] not in player.position:\n",
    "            return self.x_food, self.y_food\n",
    "        else:\n",
    "            self.food_coord(game,player)\n",
    "\n",
    "    def display_food(self, x, y, game):\n",
    "        game.gameDisplay.blit(self.image, (x, y))\n",
    "        update_screen()\n",
    "\n",
    "\n",
    "def eat(player, food, game):\n",
    "    if player.x == food.x_food and player.y == food.y_food:\n",
    "        food.food_coord(game, player)\n",
    "        player.eaten = True\n",
    "        game.score = game.score + 1\n",
    "\n",
    "\n",
    "@vectorize\n",
    "def get_record(score, record):\n",
    "        if score >= record:\n",
    "            return score\n",
    "        else:\n",
    "            return record\n",
    "\n",
    "\n",
    "def display_ui(game, score, record):\n",
    "    myfont = pygame.font.SysFont('Segoe UI', 20)\n",
    "    myfont_bold = pygame.font.SysFont('Segoe UI', 20, True)\n",
    "    text_score = myfont.render('SCORE: ', True, (0, 0, 0))\n",
    "    text_score_number = myfont.render(str(score), True, (0, 0, 0))\n",
    "    text_highest = myfont.render('HIGHEST SCORE: ', True, (0, 0, 0))\n",
    "    text_highest_number = myfont_bold.render(str(record), True, (0, 0, 0))\n",
    "    game.gameDisplay.blit(text_score, (45, 440))\n",
    "    game.gameDisplay.blit(text_score_number, (120, 440))\n",
    "    game.gameDisplay.blit(text_highest, (190, 440))\n",
    "    game.gameDisplay.blit(text_highest_number, (350, 440))\n",
    "    game.gameDisplay.blit(game.bg, (10, 10))\n",
    "    game.gameDisplay.blit(game.bg_1, (20, 100))\n",
    "\n",
    "\n",
    "def display(player, food, game, record):\n",
    "    game.gameDisplay.fill((255, 255, 255))\n",
    "    display_ui(game, game.score, record)\n",
    "    player.display_player(player.position[-1][0], player.position[-1][1], player.food, game)\n",
    "    food.display_food(food.x_food, food.y_food, game)\n",
    "\n",
    "\n",
    "def update_screen():\n",
    "    pygame.display.update()\n",
    "\n",
    "\n",
    "def initialize_game(player, game, food, agent):\n",
    "    state_init1 = agent.get_state(game, player, food)  # [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
    "    action = [1, 0, 0]\n",
    "    player.do_move(action, player.x, player.y, game, food, agent)\n",
    "    state_init2 = agent.get_state(game, player, food)\n",
    "    reward1 = agent.set_reward(player, game.crash)\n",
    "    agent.remember(state_init1, action, reward1, state_init2, game.crash)\n",
    "    agent.replay_new(agent.memory)\n",
    "\n",
    "\n",
    "def plot_seaborn(array_counter, array_score):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_score])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='score')\n",
    "    plt.show()\n",
    "    \n",
    "def run():\n",
    "    pygame.init()\n",
    "    agent = DQNAgent()\n",
    "    counter_games = 0\n",
    "    score_plot = []\n",
    "    counter_plot =[]\n",
    "    record = 0\n",
    "    print(type(record))\n",
    "    while counter_games < 150:\n",
    "        # Initialize classes\n",
    "        game = Game(440, 440)\n",
    "        player1 = game.player\n",
    "        food1 = game.food\n",
    "\n",
    "        # Perform first move\n",
    "        initialize_game(player1, game, food1, agent)\n",
    "        if display_option:\n",
    "            display(player1, food1, game, record)\n",
    "\n",
    "        while not game.crash:\n",
    "            #agent.epsilon is set to give randomness to actions\n",
    "            agent.epsilon = 60 + counter_games\n",
    "            if agent.epsilon>95:\n",
    "                agent.epsilon=90\n",
    "            #get old state\n",
    "            state_old = agent.get_state(game, player1, food1)\n",
    "            \n",
    "            #perform random actions based on agent.epsilon, or choose the action\n",
    "            if randint(0, 200) < agent.epsilon:\n",
    "                final_move = to_categorical(randint(0, 2), num_classes=3)\n",
    "            else:\n",
    "                # predict action based on the old state\n",
    "                prediction = agent.model.predict(state_old.reshape((1,11)))\n",
    "                final_move = to_categorical(np.argmax(prediction[0]), num_classes=3)\n",
    "                \n",
    "            #perform new move and get new state\n",
    "            player1.do_move(final_move, player1.x, player1.y, game, food1, agent)\n",
    "            state_new = agent.get_state(game, player1, food1)\n",
    "            \n",
    "            #set treward for the new state\n",
    "            reward = agent.set_reward(player1, game.crash)\n",
    "            \n",
    "            #train short memory base on the new action and state\n",
    "            agent.train_short_memory(state_old, final_move, reward, state_new, game.crash)\n",
    "            \n",
    "            # store the new data into a long term memory\n",
    "            agent.remember(state_old, final_move, reward, state_new, game.crash)\n",
    "            record = get_record(game.score, record)\n",
    "            if display_option:\n",
    "                display(player1, food1, game, record)\n",
    "                pygame.time.wait(speed)\n",
    "        \n",
    "        agent.replay_new(agent.memory)\n",
    "        counter_games += 1\n",
    "        #print('Game', counter_games, '      Score:', game.score)\n",
    "        score_plot.append(game.score)\n",
    "        counter_plot.append(counter_games)\n",
    "        #print('player.position: ',player.position)\n",
    "    agent.model.save_weights('weights_1.hdf5')\n",
    "    plot_seaborn(counter_plot, score_plot)\n",
    "    \n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
